{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9461073",
   "metadata": {},
   "source": [
    "# ğŸ“¦ Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dbc03",
   "metadata": {},
   "source": [
    "# Loan Payback Prediction - Kaggle Competition\n",
    "\n",
    "**Goal:** Predict the probability that a borrower will pay back their loan  \n",
    "**Best Model:** CatBoost with Optuna optimization â†’ **0.9214 AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Advanced models\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c060477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (593994, 13)\n",
      "\n",
      "Column names: ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade', 'loan_paid_back']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>annual_income</th><th>debt_to_income_ratio</th><th>credit_score</th><th>loan_amount</th><th>interest_rate</th><th>gender</th><th>marital_status</th><th>education_level</th><th>employment_status</th><th>loan_purpose</th><th>grade_subgrade</th><th>loan_paid_back</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>29367.99</td><td>0.084</td><td>736</td><td>2528.42</td><td>13.67</td><td>&quot;Female&quot;</td><td>&quot;Single&quot;</td><td>&quot;High School&quot;</td><td>&quot;Self-employed&quot;</td><td>&quot;Other&quot;</td><td>&quot;C3&quot;</td><td>1.0</td></tr><tr><td>1</td><td>22108.02</td><td>0.166</td><td>636</td><td>4593.1</td><td>12.92</td><td>&quot;Male&quot;</td><td>&quot;Married&quot;</td><td>&quot;Master&#x27;s&quot;</td><td>&quot;Employed&quot;</td><td>&quot;Debt consolidation&quot;</td><td>&quot;D3&quot;</td><td>0.0</td></tr><tr><td>2</td><td>49566.2</td><td>0.097</td><td>694</td><td>17005.15</td><td>9.76</td><td>&quot;Male&quot;</td><td>&quot;Single&quot;</td><td>&quot;High School&quot;</td><td>&quot;Employed&quot;</td><td>&quot;Debt consolidation&quot;</td><td>&quot;C5&quot;</td><td>1.0</td></tr><tr><td>3</td><td>46858.25</td><td>0.065</td><td>533</td><td>4682.48</td><td>16.1</td><td>&quot;Female&quot;</td><td>&quot;Single&quot;</td><td>&quot;High School&quot;</td><td>&quot;Employed&quot;</td><td>&quot;Debt consolidation&quot;</td><td>&quot;F1&quot;</td><td>1.0</td></tr><tr><td>4</td><td>25496.7</td><td>0.053</td><td>665</td><td>12184.43</td><td>10.21</td><td>&quot;Male&quot;</td><td>&quot;Married&quot;</td><td>&quot;High School&quot;</td><td>&quot;Employed&quot;</td><td>&quot;Other&quot;</td><td>&quot;D1&quot;</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ id  â”† annual_inc â”† debt_to_in â”† credit_sco â”† â€¦ â”† employment â”† loan_purpo â”† grade_sub â”† loan_paid â”‚\n",
       "â”‚ --- â”† ome        â”† come_ratio â”† re         â”†   â”† _status    â”† se         â”† grade     â”† _back     â”‚\n",
       "â”‚ i64 â”† ---        â”† ---        â”† ---        â”†   â”† ---        â”† ---        â”† ---       â”† ---       â”‚\n",
       "â”‚     â”† f64        â”† f64        â”† i64        â”†   â”† str        â”† str        â”† str       â”† f64       â”‚\n",
       "â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0   â”† 29367.99   â”† 0.084      â”† 736        â”† â€¦ â”† Self-emplo â”† Other      â”† C3        â”† 1.0       â”‚\n",
       "â”‚     â”†            â”†            â”†            â”†   â”† yed        â”†            â”†           â”†           â”‚\n",
       "â”‚ 1   â”† 22108.02   â”† 0.166      â”† 636        â”† â€¦ â”† Employed   â”† Debt conso â”† D3        â”† 0.0       â”‚\n",
       "â”‚     â”†            â”†            â”†            â”†   â”†            â”† lidation   â”†           â”†           â”‚\n",
       "â”‚ 2   â”† 49566.2    â”† 0.097      â”† 694        â”† â€¦ â”† Employed   â”† Debt conso â”† C5        â”† 1.0       â”‚\n",
       "â”‚     â”†            â”†            â”†            â”†   â”†            â”† lidation   â”†           â”†           â”‚\n",
       "â”‚ 3   â”† 46858.25   â”† 0.065      â”† 533        â”† â€¦ â”† Employed   â”† Debt conso â”† F1        â”† 1.0       â”‚\n",
       "â”‚     â”†            â”†            â”†            â”†   â”†            â”† lidation   â”†           â”†           â”‚\n",
       "â”‚ 4   â”† 25496.7    â”† 0.053      â”† 665        â”† â€¦ â”† Employed   â”† Other      â”† D1        â”† 1.0       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pl.read_csv(\"data/train.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"\\nColumn names: {train_df.columns}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95168c63",
   "metadata": {},
   "source": [
    "# Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset shape: (254569, 12)\n",
      "Sample submission shape: (254569, 2)\n",
      "\n",
      "Test columns: ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
      "Sample submission columns: ['id', 'loan_paid_back']\n",
      "\n",
      "Null values in train data:\n",
      "shape: (1, 13)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id  â”† annual_inc â”† debt_to_in â”† credit_sco â”† â€¦ â”† employment â”† loan_purpo â”† grade_sub â”† loan_paid â”‚\n",
      "â”‚ --- â”† ome        â”† come_ratio â”† re         â”†   â”† _status    â”† se         â”† grade     â”† _back     â”‚\n",
      "â”‚ u32 â”† ---        â”† ---        â”† ---        â”†   â”† ---        â”† ---        â”† ---       â”† ---       â”‚\n",
      "â”‚     â”† u32        â”† u32        â”† u32        â”†   â”† u32        â”† u32        â”† u32       â”† u32       â”‚\n",
      "â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0          â”† 0          â”† 0          â”† â€¦ â”† 0          â”† 0          â”† 0         â”† 0         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "test_df = pl.read_csv(\"data/test.csv\")\n",
    "sample_submission = pl.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "train_pd = train_df.to_pandas()\n",
    "test_pd = test_df.to_pandas()\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"  Training: {train_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")\n",
    "print(f\"  Target distribution: {train_pd['loan_paid_back'].mean():.3f}\")\n",
    "print(f\"\\nNo missing values: {train_df.null_count().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ee171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created:\n",
      "   loan_to_income_ratio  income_per_debt_ratio  credit_score_normalized grade  \\\n",
      "0              0.086094          349614.766491                 0.792727     C   \n",
      "1              0.207757          133180.041084                 0.610909     D   \n",
      "2              0.343080          510986.484675                 0.716364     C   \n",
      "3              0.099929          720885.063307                 0.423636     F   \n",
      "4              0.477883          481060.734703                 0.663636     D   \n",
      "\n",
      "   subgrade credit_score_bin interest_rate_bin  \n",
      "0         3             Good              High  \n",
      "1         3             Fair            Medium  \n",
      "2         5             Good               Low  \n",
      "3         1             Poor         Very_High  \n",
      "4         1             Fair            Medium  \n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"Create comprehensive feature engineering pipeline\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Basic engineered features\n",
    "    df_processed['loan_to_income_ratio'] = df_processed['loan_amount'] / df_processed['annual_income']\n",
    "    df_processed['income_per_debt_ratio'] = df_processed['annual_income'] / (df_processed['debt_to_income_ratio'] + 1e-6)\n",
    "    df_processed['credit_score_normalized'] = (df_processed['credit_score'] - 300) / (850 - 300)\n",
    "    \n",
    "    # Extract grade components\n",
    "    df_processed['grade'] = df_processed['grade_subgrade'].str[0]\n",
    "    df_processed['subgrade'] = df_processed['grade_subgrade'].str[1:].astype(int)\n",
    "    df_processed['grade_numeric'] = df_processed['grade'].map({'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7})\n",
    "    \n",
    "    # Binning features\n",
    "    df_processed['credit_score_bin'] = pd.cut(df_processed['credit_score'], \n",
    "                                            bins=[0, 580, 670, 740, 850], \n",
    "                                            labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
    "    df_processed['interest_rate_bin'] = pd.cut(df_processed['interest_rate'], \n",
    "                                             bins=[0, 10, 13, 16, 25], \n",
    "                                             labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "    \n",
    "    # Advanced financial metrics\n",
    "    df_processed['risk_score'] = (df_processed['debt_to_income_ratio'] * df_processed['interest_rate']) / (df_processed['credit_score'] / 100)\n",
    "    df_processed['payment_capacity'] = df_processed['annual_income'] / (df_processed['loan_amount'] * df_processed['interest_rate'] / 100)\n",
    "    df_processed['debt_burden'] = df_processed['loan_amount'] * df_processed['debt_to_income_ratio']\n",
    "    df_processed['affordability_index'] = df_processed['annual_income'] / (df_processed['loan_amount'] + df_processed['annual_income'] * df_processed['debt_to_income_ratio'])\n",
    "    \n",
    "    # Credit and income interactions\n",
    "    df_processed['credit_income_interaction'] = df_processed['credit_score'] * np.log1p(df_processed['annual_income'])\n",
    "    df_processed['credit_loan_ratio'] = df_processed['credit_score'] / df_processed['loan_amount'] * 1000\n",
    "    df_processed['income_stability'] = df_processed['annual_income'] / (df_processed['debt_to_income_ratio'] + 1e-6)\n",
    "    df_processed['grade_risk'] = df_processed['grade_numeric'] * df_processed['interest_rate']\n",
    "    df_processed['subgrade_risk'] = df_processed['subgrade'] * df_processed['debt_to_income_ratio']\n",
    "    \n",
    "    # Log transformations\n",
    "    df_processed['log_annual_income'] = np.log1p(df_processed['annual_income'])\n",
    "    df_processed['log_loan_amount'] = np.log1p(df_processed['loan_amount'])\n",
    "    df_processed['sqrt_credit_score'] = np.sqrt(df_processed['credit_score'])\n",
    "    \n",
    "    # Categorical combinations\n",
    "    df_processed['credit_employment_combo'] = df_processed['credit_score_bin'].astype(str) + '_' + df_processed['employment_status'].astype(str)\n",
    "    df_processed['grade_purpose_combo'] = df_processed['grade'].astype(str) + '_' + df_processed['loan_purpose'].astype(str)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply feature engineering\n",
    "train_processed = feature_engineering(train_pd)\n",
    "test_processed = feature_engineering(test_pd)\n",
    "\n",
    "print(f\"Features created: {train_processed.shape[1]} total features\")\n",
    "new_features = set(train_processed.columns) - set(train_pd.columns)\n",
    "print(f\"New features: {len(new_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32cf7d8",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2241d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (593994, 17)\n",
      "Test data shape: (254569, 17)\n",
      "Features: ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'loan_to_income_ratio', 'income_per_debt_ratio', 'credit_score_normalized', 'subgrade', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade', 'credit_score_bin', 'interest_rate_bin']\n",
      "Target distribution: loan_paid_back\n",
      "1.0    474494\n",
      "0.0    119500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def prepare_features(df, fit_encoders=True, encoders=None):\n",
    "    \"\"\"Prepare features for modeling with comprehensive encoding\"\"\"\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # High cardinality categorical features\n",
    "    high_card_cats = ['credit_employment_combo', 'grade_purpose_combo']\n",
    "    \n",
    "    # Regular categorical features\n",
    "    categorical_features = ['gender', 'marital_status', 'education_level', 'employment_status', \n",
    "                           'loan_purpose', 'grade_subgrade', 'credit_score_bin', 'interest_rate_bin', 'grade']\n",
    "    \n",
    "    # Numerical features\n",
    "    numerical_features = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', \n",
    "                         'interest_rate', 'loan_to_income_ratio', 'income_per_debt_ratio', \n",
    "                         'credit_score_normalized', 'subgrade', 'risk_score', 'payment_capacity',\n",
    "                         'debt_burden', 'affordability_index', 'credit_income_interaction',\n",
    "                         'credit_loan_ratio', 'income_stability', 'grade_numeric', 'grade_risk',\n",
    "                         'subgrade_risk', 'log_annual_income', 'log_loan_amount', 'sqrt_credit_score']\n",
    "    \n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "    \n",
    "    # Frequency encoding for high cardinality features\n",
    "    for feature in high_card_cats:\n",
    "        if fit_encoders:\n",
    "            freq_map = df_model[feature].value_counts().to_dict()\n",
    "            encoders[f'{feature}_freq'] = freq_map\n",
    "            df_model[f'{feature}_freq'] = df_model[feature].map(freq_map).fillna(0)\n",
    "        else:\n",
    "            freq_map = encoders.get(f'{feature}_freq', {})\n",
    "            df_model[f'{feature}_freq'] = df_model[feature].map(freq_map).fillna(0)\n",
    "    \n",
    "    # Label encoding for categorical features\n",
    "    for feature in categorical_features:\n",
    "        if fit_encoders:\n",
    "            le = LabelEncoder()\n",
    "            df_model[feature] = le.fit_transform(df_model[feature].astype(str))\n",
    "            encoders[feature] = le\n",
    "        else:\n",
    "            if feature in encoders:\n",
    "                unique_vals = set(df_model[feature].astype(str).unique())\n",
    "                known_vals = set(encoders[feature].classes_)\n",
    "                unseen_vals = unique_vals - known_vals\n",
    "                if unseen_vals:\n",
    "                    df_model[feature] = df_model[feature].astype(str).apply(\n",
    "                        lambda x: x if x in known_vals else encoders[feature].classes_[0]\n",
    "                    )\n",
    "                df_model[feature] = encoders[feature].transform(df_model[feature].astype(str))\n",
    "    \n",
    "    # Select final features\n",
    "    freq_features = [f'{feat}_freq' for feat in high_card_cats]\n",
    "    final_features = numerical_features + categorical_features + freq_features\n",
    "    \n",
    "    return df_model[final_features], encoders\n",
    "\n",
    "# Prepare features\n",
    "X, encoders = prepare_features(train_processed, fit_encoders=True)\n",
    "y = train_processed['loan_paid_back']\n",
    "X_test, _ = prepare_features(test_processed, fit_encoders=False, encoders=encoders)\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training features shape: {X.shape}\")\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23f1a3",
   "metadata": {},
   "source": [
    "# Model Training & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:07:22,459] A new study created in memory with name: no-name-52ea6ed1-feab-45a5-b98e-c6ddc224efee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.908986:   5%|â–Œ         | 1/20 [00:18<05:56, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:07:41,226] Trial 0 finished with value: 0.9089859855849388 and parameters: {'iterations': 437, 'depth': 10, 'learning_rate': 0.22227824312530747, 'l2_leaf_reg': 6.387926357773329, 'random_strength': 0.24041677639819287, 'bagging_temperature': 0.2403950683025824, 'border_count': 45}. Best is trial 0 with value: 0.9089859855849388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.910597:  10%|â–ˆ         | 2/20 [00:40<06:06, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:08:02,721] Trial 1 finished with value: 0.9105967864271114 and parameters: {'iterations': 880, 'depth': 8, 'learning_rate': 0.21534104756085318, 'l2_leaf_reg': 1.185260448662222, 'random_strength': 0.9729188669457949, 'bagging_temperature': 0.8491983767203796, 'border_count': 79}. Best is trial 1 with value: 0.9105967864271114.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.917309:  15%|â–ˆâ–Œ        | 3/20 [00:45<03:48, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:08:07,945] Trial 2 finished with value: 0.9173094870604781 and parameters: {'iterations': 263, 'depth': 5, 'learning_rate': 0.09823025045826593, 'l2_leaf_reg': 5.72280788469014, 'random_strength': 0.4887505167779042, 'bagging_temperature': 0.36210622617823773, 'border_count': 169}. Best is trial 2 with value: 0.9173094870604781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917359:  20%|â–ˆâ–ˆ        | 4/20 [00:50<02:40, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:08:12,770] Trial 3 finished with value: 0.9173592382581949 and parameters: {'iterations': 225, 'depth': 6, 'learning_rate': 0.11624493455517058, 'l2_leaf_reg': 5.104629857953324, 'random_strength': 0.8066583652537123, 'bagging_temperature': 0.2797064039425238, 'border_count': 147}. Best is trial 3 with value: 0.9173592382581949.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:02<02:40, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:08:24,573] Trial 4 finished with value: 0.9214267505100445 and parameters: {'iterations': 633, 'depth': 4, 'learning_rate': 0.1861880070514171, 'l2_leaf_reg': 2.5347171131856236, 'random_strength': 0.15854643368675156, 'bagging_temperature': 0.953996983528, 'border_count': 248}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:19<03:03, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:08:42,407] Trial 5 finished with value: 0.9176322618440278 and parameters: {'iterations': 828, 'depth': 6, 'learning_rate': 0.03832491306185132, 'l2_leaf_reg': 7.158097238609412, 'random_strength': 0.49613724436564116, 'bagging_temperature': 0.20983441136030095, 'border_count': 142}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:26<02:22, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:08:49,030] Trial 6 finished with value: 0.9166260694424255 and parameters: {'iterations': 130, 'depth': 10, 'learning_rate': 0.0850461946640049, 'l2_leaf_reg': 6.962700559185838, 'random_strength': 0.3805399684804699, 'bagging_temperature': 0.5680612190600297, 'border_count': 154}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:41<02:25, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:09:03,635] Trial 7 finished with value: 0.9185211452069656 and parameters: {'iterations': 266, 'depth': 10, 'learning_rate': 0.2347885187747232, 'l2_leaf_reg': 9.455490474077703, 'random_strength': 0.905344615384884, 'bagging_temperature': 0.6381099809299766, 'border_count': 238}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:44<01:44,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:09:07,355] Trial 8 finished with value: 0.9131697868524479 and parameters: {'iterations': 179, 'depth': 5, 'learning_rate': 0.023115913784056037, 'l2_leaf_reg': 3.927972976869379, 'random_strength': 0.4498095607205338, 'bagging_temperature': 0.34421412859650635, 'border_count': 217}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [01:53<01:32,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:09:16,177] Trial 9 finished with value: 0.9209037350918909 and parameters: {'iterations': 421, 'depth': 5, 'learning_rate': 0.16738186411589204, 'l2_leaf_reg': 2.2683180247728636, 'random_strength': 0.8219772826786357, 'bagging_temperature': 0.16709557931179375, 'border_count': 253}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [02:06<01:33, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:09:29,088] Trial 10 finished with value: 0.9207294052902136 and parameters: {'iterations': 675, 'depth': 4, 'learning_rate': 0.29116576212848105, 'l2_leaf_reg': 3.2466279775707045, 'random_strength': 0.11308678764612795, 'bagging_temperature': 0.9168983157542662, 'border_count': 195}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:16<01:22, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:09:39,305] Trial 11 finished with value: 0.9209374692609773 and parameters: {'iterations': 542, 'depth': 4, 'learning_rate': 0.16967550564060227, 'l2_leaf_reg': 1.122488171413103, 'random_strength': 0.7274403085060852, 'bagging_temperature': 0.7666332755270974, 'border_count': 254}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:29<01:16, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:09:51,752] Trial 12 finished with value: 0.920345479388988 and parameters: {'iterations': 644, 'depth': 4, 'learning_rate': 0.1686655186001, 'l2_leaf_reg': 1.038567007063697, 'random_strength': 0.6733297974046909, 'bagging_temperature': 0.761435931620551, 'border_count': 209}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:45<01:14, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:10:07,581] Trial 13 finished with value: 0.9207520702146184 and parameters: {'iterations': 575, 'depth': 8, 'learning_rate': 0.13733640092436256, 'l2_leaf_reg': 2.613227502415608, 'random_strength': 0.6598164279756535, 'bagging_temperature': 0.9955465841211626, 'border_count': 255}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [03:00<01:06, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:10:22,860] Trial 14 finished with value: 0.9172348637911056 and parameters: {'iterations': 781, 'depth': 4, 'learning_rate': 0.18913676036556212, 'l2_leaf_reg': 3.9705072688621197, 'random_strength': 0.639766995366067, 'bagging_temperature': 0.7405293748993315, 'border_count': 101}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [03:26<01:09, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:10:49,340] Trial 15 finished with value: 0.9160444305884154 and parameters: {'iterations': 987, 'depth': 7, 'learning_rate': 0.2864345481322442, 'l2_leaf_reg': 2.1626190183232197, 'random_strength': 0.31645162857705883, 'bagging_temperature': 0.7999404431562471, 'border_count': 183}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [03:38<00:46, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:11:01,122] Trial 16 finished with value: 0.9204346003443168 and parameters: {'iterations': 474, 'depth': 6, 'learning_rate': 0.24964252563931805, 'l2_leaf_reg': 4.490191397725659, 'random_strength': 0.11864321041779169, 'bagging_temperature': 0.6541803608473044, 'border_count': 222}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:52<00:30, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:11:15,119] Trial 17 finished with value: 0.918470487077188 and parameters: {'iterations': 694, 'depth': 4, 'learning_rate': 0.18725103746238383, 'l2_leaf_reg': 1.6241128644754506, 'random_strength': 0.6009535996389941, 'bagging_temperature': 0.9886943731160998, 'border_count': 125}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [04:06<00:14, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:11:29,023] Trial 18 finished with value: 0.9204667375722642 and parameters: {'iterations': 536, 'depth': 5, 'learning_rate': 0.1423294288603213, 'l2_leaf_reg': 2.9136732663727702, 'random_strength': 0.7608796812995494, 'bagging_temperature': 0.4498922753077476, 'border_count': 232}. Best is trial 4 with value: 0.9214267505100445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.921427: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [04:17<00:00, 12.85s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 10:11:39,533] Trial 19 finished with value: 0.9178878623699516 and parameters: {'iterations': 348, 'depth': 7, 'learning_rate': 0.06511842795285187, 'l2_leaf_reg': 8.663656475710512, 'random_strength': 0.25901840658144304, 'bagging_temperature': 0.9061604145189799, 'border_count': 197}. Best is trial 4 with value: 0.9214267505100445.\n",
      "Best CatBoost AUC: 0.9214\n",
      "Best CatBoost params: {'iterations': 633, 'depth': 4, 'learning_rate': 0.1861880070514171, 'l2_leaf_reg': 2.5347171131856236, 'random_strength': 0.15854643368675156, 'bagging_temperature': 0.953996983528, 'border_count': 248}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x36371c870>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CatBoost Hyperparameter Optimization with Optuna\n",
    "\n",
    "def optimize_catboost(trial):\n",
    "    \"\"\"Optimize CatBoost hyperparameters using Optuna\"\"\"\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 1.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.1, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'verbose': False,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "    \n",
    "    model = cb.CatBoostClassifier(**params)\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "print(\"ğŸ” Optimizing CatBoost with Optuna (20 trials)...\")\n",
    "study_catboost = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_catboost.optimize(optimize_catboost, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… Best CatBoost AUC: {study_catboost.best_value:.4f}\")\n",
    "print(f\"Best parameters: {study_catboost.best_params}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_catboost_params = study_catboost.best_params\n",
    "best_catboost_params.update({'verbose': False, 'random_seed': 42})\n",
    "catboost_model = cb.CatBoostClassifier(**best_catboost_params)\n",
    "catboost_model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Evaluate on validation set\n",
    "catboost_val_pred = catboost_model.predict_proba(X_val)[:, 1]\n",
    "catboost_auc = roc_auc_score(y_val, catboost_val_pred)\n",
    "print(f\"Final CatBoost validation AUC: {catboost_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51365f",
   "metadata": {},
   "source": [
    "# TabNet (Attention-Based Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ TRAINING TABNET - Attention-Based Deep Learning for Tabular Data\n",
      "================================================================================\n",
      "ğŸ¯ Training TabNet Model...\n",
      "TabNet uses sequential attention mechanism to select important features\n",
      "Training TabNet with attention mechanism...\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_auc = 0.913\n",
      "âœ… TabNet AUC: 0.9130\n",
      "\n",
      "Top 10 Features by TabNet Attention:\n",
      "                feature  importance\n",
      "      sqrt_credit_score    0.411894\n",
      "   debt_to_income_ratio    0.359061\n",
      "      employment_status    0.183223\n",
      "credit_score_normalized    0.031117\n",
      "          annual_income    0.011878\n",
      "            loan_amount    0.002801\n",
      "       income_stability    0.000027\n",
      "                 gender    0.000000\n",
      "         marital_status    0.000000\n",
      "        education_level    0.000000\n",
      "\n",
      "ğŸ“Š PERFORMANCE COMPARISON:\n",
      "Original CatBoost:  0.9214 AUC\n",
      "TabNet Model:       0.9130 AUC\n",
      "ğŸ“‰ TabNet is 8.4 points behind CatBoost\n",
      "CatBoost remains the best model\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train TabNet Model with Attention Mechanism\n",
    "\n",
    "def train_tabnet_model():\n",
    "    \"\"\"Train TabNet - Attention-based neural network for tabular data\"\"\"\n",
    "    print(\"ğŸ¯ Training TabNet with attention mechanism...\")\n",
    "    \n",
    "    # Prepare data (TabNet needs numpy arrays)\n",
    "    X_train_np = X_train.values\n",
    "    X_val_np = X_val.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = y_train.values\n",
    "    y_val_np = y_val.values\n",
    "    \n",
    "    # TabNet configuration\n",
    "    tabnet_params = {\n",
    "        \"n_d\": 64,  # Width of decision prediction layer\n",
    "        \"n_a\": 64,  # Width of attention embedding\n",
    "        \"n_steps\": 5,  # Number of sequential attention steps\n",
    "        \"gamma\": 1.5,  # Coefficient for feature reusage\n",
    "        \"n_independent\": 2,  # Independent GLU layers\n",
    "        \"n_shared\": 2,  # Shared GLU layers\n",
    "        \"momentum\": 0.3,  # Batch norm momentum\n",
    "        \"mask_type\": \"entmax\",  # Attention mask type\n",
    "        \"lambda_sparse\": 1e-4,  # Sparsity loss coefficient\n",
    "        \"seed\": 42,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "    \n",
    "    # Initialize and train\n",
    "    tabnet_model = TabNetClassifier(**tabnet_params)\n",
    "    tabnet_model.fit(\n",
    "        X_train_np, y_train_np,\n",
    "        eval_set=[(X_val_np, y_val_np)],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=100,\n",
    "        patience=20,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_pred_tabnet = tabnet_model.predict_proba(X_val_np)[:, 1]\n",
    "    test_pred_tabnet = tabnet_model.predict_proba(X_test_np)[:, 1]\n",
    "    tabnet_auc = roc_auc_score(y_val_np, val_pred_tabnet)\n",
    "    \n",
    "    print(f\"âœ… TabNet AUC: {tabnet_auc:.4f}\")\n",
    "    \n",
    "    # Feature importance from attention\n",
    "    feature_importances = tabnet_model.feature_importances_\n",
    "    feature_imp_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': feature_importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Features by TabNet Attention:\")\n",
    "    print(feature_imp_df.head(10).to_string(index=False))\n",
    "    \n",
    "    return tabnet_model, test_pred_tabnet, tabnet_auc, val_pred_tabnet\n",
    "\n",
    "# Train TabNet\n",
    "tabnet_model, tabnet_predictions, tabnet_auc, tabnet_val_pred = train_tabnet_model()\n",
    "\n",
    "print(\"\\nğŸ“Š Model Comparison:\")\n",
    "print(f\"CatBoost:  {catboost_auc:.4f} AUC\")\n",
    "print(f\"TabNet:    {tabnet_auc:.4f} AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1354c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Creating Hybrid Ensemble: CatBoost + TabNet\n",
      "================================================================================\n",
      "Testing different ensemble weights...\n",
      "\n",
      "Best Ensemble Configuration:\n",
      "CatBoost weight: 0.95\n",
      "TabNet weight: 0.05\n",
      "Ensemble AUC: 0.9213\n",
      "\n",
      "âœ… Final hybrid submission created!\n",
      "File: submission_final_hybrid.csv\n",
      "\n",
      "ğŸ“Š FINAL MODEL COMPARISON:\n",
      "1. Original CatBoost:    0.9214 AUC\n",
      "2. TabNet:               0.9130 AUC\n",
      "3. Hybrid Ensemble:      0.9213 AUC\n",
      "\n",
      "âœ… CatBoost remains the champion at 0.9214 AUC\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ensemble: Combine CatBoost and TabNet\n",
    "\n",
    "print(\"ğŸ¯ Creating Hybrid Ensemble: CatBoost + TabNet\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find optimal ensemble weights\n",
    "best_ensemble_auc = 0\n",
    "best_weight = 0\n",
    "\n",
    "for catboost_weight in np.arange(0.5, 1.0, 0.05):\n",
    "    tabnet_weight = 1 - catboost_weight\n",
    "    ensemble_val = catboost_weight * catboost_val_pred + tabnet_weight * tabnet_val_pred\n",
    "    ensemble_auc = roc_auc_score(y_val, ensemble_val)\n",
    "    \n",
    "    if ensemble_auc > best_ensemble_auc:\n",
    "        best_ensemble_auc = ensemble_auc\n",
    "        best_weight = catboost_weight\n",
    "\n",
    "print(f\"Best Ensemble: {best_weight:.0%} CatBoost + {1-best_weight:.0%} TabNet\")\n",
    "print(f\"Ensemble AUC: {best_ensemble_auc:.4f}\")\n",
    "\n",
    "# Create ensemble predictions\n",
    "catboost_test_pred = catboost_model.predict_proba(X_test)[:, 1]\n",
    "hybrid_predictions = best_weight * catboost_test_pred + (1-best_weight) * tabnet_predictions\n",
    "\n",
    "print(\"\\nğŸ“Š Final Model Rankings:\")\n",
    "print(f\"1. CatBoost (Optimized):  {catboost_auc:.4f} AUC\")\n",
    "print(f\"2. Hybrid Ensemble:       {best_ensemble_auc:.4f} AUC\")\n",
    "print(f\"3. TabNet:                {tabnet_auc:.4f} AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831280f",
   "metadata": {},
   "source": [
    "# Generate Predictions & Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b43f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final predictions with post-processing\n",
    "\n",
    "def apply_post_processing(predictions, target_mean):\n",
    "    \"\"\"Apply calibration to match training distribution\"\"\"\n",
    "    processed = np.clip(predictions, 0.002, 0.998)\n",
    "    current_mean = processed.mean()\n",
    "    adjustment = target_mean / current_mean\n",
    "    processed = processed * adjustment\n",
    "    processed = np.clip(processed, 0.001, 0.999)\n",
    "    return processed\n",
    "\n",
    "# Get training target mean\n",
    "train_target_mean = y.mean()\n",
    "\n",
    "# Process predictions\n",
    "hybrid_processed = apply_post_processing(hybrid_predictions, train_target_mean)\n",
    "catboost_test_pred = catboost_model.predict_proba(X_test)[:, 1]\n",
    "catboost_processed = apply_post_processing(catboost_test_pred, train_target_mean)\n",
    "\n",
    "# Create submissions\n",
    "submission_hybrid = pd.DataFrame({\n",
    "    'id': test_processed['id'],\n",
    "    'loan_paid_back': hybrid_processed\n",
    "})\n",
    "\n",
    "submission_catboost = pd.DataFrame({\n",
    "    'id': test_processed['id'],\n",
    "    'loan_paid_back': catboost_processed\n",
    "})\n",
    "\n",
    "# Save files\n",
    "submission_hybrid.to_csv('submission_hybrid.csv', index=False)\n",
    "submission_catboost.to_csv('submission_catboost.csv', index=False)\n",
    "\n",
    "print(\"âœ… Submission files created!\")\n",
    "print(\"\\nğŸ“ Files:\")\n",
    "print(f\"  â€¢ submission_catboost.csv (CatBoost only - {catboost_auc:.4f} AUC)\")\n",
    "print(\"  â€¢ submission_hybrid.csv (CatBoost + TabNet ensemble)\")\n",
    "\n",
    "print(\"\\nğŸ“Š Prediction Statistics:\")\n",
    "print(f\"Training mean: {train_target_mean:.4f}\")\n",
    "print(f\"CatBoost mean: {catboost_processed.mean():.4f}\")\n",
    "print(f\"Hybrid mean:   {hybrid_processed.mean():.4f}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Recommended: submission_catboost.csv\")\n",
    "print(f\"   (Best validation AUC: {catboost_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d90a8",
   "metadata": {},
   "source": [
    "# Results Summary\n",
    "\n",
    "## Final Model Performance\n",
    "\n",
    "| Model | Validation AUC |\n",
    "|-------|---------------|\n",
    "| **CatBoost (Optimized)** | **0.9214** |\n",
    "| Hybrid Ensemble | 0.9213 |\n",
    "| TabNet | 0.9130 |\n",
    "\n",
    "## Top 5 Features (CatBoost)\n",
    "1. Employment Status (54.9%)\n",
    "2. Debt-to-Income Ratio (14.1%)\n",
    "3. Credit-Employment Combo (6.1%)\n",
    "4. Sqrt Credit Score (6.0%)\n",
    "5. Risk Score (5.8%)\n",
    "\n",
    "## Submission Files\n",
    "- `submission_catboost.csv` - **Recommended** (0.9214 AUC)\n",
    "- `submission_hybrid.csv` - Ensemble approach\n",
    "\n",
    "See README.md for full methodology and insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
